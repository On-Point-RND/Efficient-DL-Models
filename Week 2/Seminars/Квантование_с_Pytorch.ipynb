{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EENlZvOtPDZ6"
   },
   "source": [
    "# Квантование с Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbiiMcdNJI--"
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch==1.5.0 torchvision==1.6.0\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.quantization\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCaMDWYArEXO"
   },
   "source": [
    "Загрузим данные MNIST для обучения и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paBliKgoU6Rm"
   },
   "outputs": [],
   "source": [
    "args={}\n",
    "args['batch_size']=500\n",
    "args['test_batch_size']=500\n",
    "args['epochs']=5  #The number of Epochs is the number of times you go through the full dataset.\n",
    "args['lr']=0.005 #Learning rate is how fast it will decend.\n",
    "args['seed']=1 #random seed\n",
    "args['log_interval']=20\n",
    "args['cuda']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5UuOjjrnogR",
    "outputId": "49e3b51d-682b-4642-d5ad-7557e5306a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 35224429.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 1132130.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 9426767.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 11658830.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=args['test_batch_size'],\n",
    "                                          shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=args['test_batch_size'],\n",
    "                                         shuffle=False, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG5qXPDxnUnj"
   },
   "source": [
    "Определите некоторые вспомогательные функции и классы, которые помогут нам отслеживать статистику и точность данных обучения/тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WetzHpQybN1k"
   },
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\" Prints the real size of the model \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "def load_model(quantized_model, model):\n",
    "    \"\"\" Loads in the weights into an object meant for quantization \"\"\"\n",
    "    state_dict = model.state_dict()\n",
    "    model = model.to('cpu')\n",
    "    quantized_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kgl8VuSAUsDA"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, loss_fn, optimizer, train_loader):\n",
    "    model.train()\n",
    "    if  args['cuda']:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.to('cpu')\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print out the loss periodically.\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    if  args['cuda']:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.to('cpu')\n",
    "\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)  Time: {}\\n'.format(correct, len(test_loader.dataset),acc, end - start))\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, test_loader):\n",
    "    if args['cuda']:\n",
    "        model.cuda()\n",
    "\n",
    "    history = []\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "\n",
    "        train(model, epoch, loss_fn, optimizer, train_loader)\n",
    "        acc = test(model, test_loader)\n",
    "        history.append(acc)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyHUiOp8T411"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, q=False):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, stride=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3, stride=5, bias=False)\n",
    "        self.fc1 = nn.Linear(32, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "        self.q = q\n",
    "        if self.q:\n",
    "            self.quant = QuantStub()\n",
    "            self.dequant = DeQuantStub()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.q:\n",
    "          x = self.quant(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        output = self.fc2(x)\n",
    "\n",
    "        if self.q:\n",
    "          output = self.dequant(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l62CkyIwtSOv"
   },
   "source": [
    "Определим простую CNN, которая классифицирует изображения MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9_LdxSTb3BJ",
    "outputId": "c017d805-6e2e-4615-d724-c8e17f841e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.176938\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(q=False)\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HixhBHaqtmZU",
    "outputId": "653d7538-75c6-492e-cdac-1c0fdf98a024"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309618\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.267061\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.911802\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.785304\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.686944\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.587097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 8361/10000 (84%)  Time: 2.3063459396362305\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.489893\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.703286\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.545334\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.457020\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.468492\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.435362\n",
      "\n",
      "Test set:  Accuracy: 8679/10000 (87%)  Time: 2.345824718475342\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.425479\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.421131\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.401944\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.415436\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.350327\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.436896\n",
      "\n",
      "Test set:  Accuracy: 8777/10000 (88%)  Time: 2.244311571121216\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.407081\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.346416\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.453229\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.354816\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.394839\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.337369\n",
      "\n",
      "Test set:  Accuracy: 8756/10000 (88%)  Time: 2.3323636054992676\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.474877\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.389702\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.434510\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.392052\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.403296\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.364162\n",
      "\n",
      "Test set:  Accuracy: 8883/10000 (89%)  Time: 2.3445191383361816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = train_and_eval(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Lp-ElDsrKua"
   },
   "source": [
    "### Post-training quantization\n",
    "\n",
    "Определим новую архитектуру квантовой сети, в которой мы также определим заглушки квантования и деквантования, которые будут важны в начале и в конце.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-nQWDXrhItv",
    "outputId": "8b824830-d873-4dbc-f3ef-62732c37d15e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 8883/10000 (89%)  Time: 2.3049731254577637\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(88.8300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodel = SimpleCNN(q=True)\n",
    "\n",
    "load_model(qmodel, model)\n",
    "\n",
    "test(qmodel, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQQRNAEGYVUe",
    "outputId": "e8cdaeca-5526-447f-bfb6-f95cc50d2053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.177066\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiaQkj6wJuC6"
   },
   "source": [
    "Статическое квантование после обучения включает в себя не только преобразование весов из числа с плавающей запятой в целое число, как при динамическом квантовании, но и выполнение дополнительных\n",
    "этап первой подачи пакетов данных через сеть и вычисления результирующих распределений различных активаций (в частности,\n",
    "это делается путем вставки модулей наблюдателей в разные\n",
    "точки, записывающие эти данные). Эти распределения затем используются для определения того, как конкретно следует квантовать различные активации.\n",
    "время вывода (простым методом было бы просто разделить весь диапазон активаций на 256 уровней.\n",
    "Важно отметить, что этот дополнительный шаг позволяет нам передавать квантованные значения между операциями вместо преобразования этих значений в числа с плавающей запятой, а затем обратно в целые числа между каждой операцией.\n",
    "что приводит к значительному ускорению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctrtCLGZALGV",
    "outputId": "26ed9550-0fb6-40cd-9f82-0305caec446f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n"
     ]
    }
   ],
   "source": [
    "qmodel.qconfig = torch.quantization.default_qconfig\n",
    "print(qmodel.qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-ZaMV4bUb6-",
    "outputId": "165cc74e-29a5-4748-cf65-97cf5fa8d2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " Conv1: After observer insertion \n",
      "\n",
      " Conv2d(\n",
      "  1, 8, kernel_size=(3, 3), stride=(1, 1), bias=False\n",
      "  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      ")\n",
      "\n",
      "Test set:  Accuracy: 53107/60000 (89%)  Time: 15.302648782730103\n",
      "\n",
      "Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " Conv1: After fusion and quantization \n",
      "\n",
      " QuantizedConv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), scale=0.031006284058094025, zero_point=64, bias=False)\n",
      "Size of model after quantization\n",
      "Size (MB): 0.05201\n"
     ]
    }
   ],
   "source": [
    "# qmodel.to('cpu')\n",
    "args['cuda']=False\n",
    "\n",
    "qmodel.qconfig = torch.quantization.default_qconfig\n",
    "print(qmodel.qconfig)\n",
    "\n",
    "torch.quantization.prepare(qmodel, inplace=True)\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "print('\\n Conv1: After observer insertion \\n\\n', qmodel.conv1)\n",
    "\n",
    "\n",
    "test(qmodel, train_loader)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "torch.quantization.convert(qmodel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Conv1: After fusion and quantization \\n\\n', qmodel.conv1)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxBwCjf41qPz",
    "outputId": "9bf12610-7f38-476c-935f-52c5e5b9cac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:  Accuracy: 8885/10000 (89%)  Time: 3.023811101913452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args['cuda']=False\n",
    "\n",
    "a = test(qmodel, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXMz5kK43OWv",
    "outputId": "4b260c71-0e03-4495-b27b-66742e85fdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.05201\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEU8Ck0KBlf5",
    "outputId": "2459e43b-dc4e-4c00-c458-eaba30b9b578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.176938\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
