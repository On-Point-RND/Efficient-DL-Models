{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m9XtcYjezMLh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание # 2\n",
        "\n",
        "# Тензорные и Матричные Разложения\n",
        "\n",
        "В данном ноутбуке вам предстоит применять матричные и тензорные разложения для оптимизации модели по количеству хранимых параметров.\n",
        "Для начала мы познакомимся с тем как устроен BERT и применим его на задаче SST2 для оценки сентимента.\n",
        "Далее предстоит применять SVD к слоям берта с целью их сжатия, и в конечном итоге попробовать тензорные разложения"
      ],
      "metadata": {
        "id": "Cn9TCK1QVh4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1 (25 Баллов всего)\n",
        "В данном задании вам предлагается использовать SVD, чтобы сэкономить параметров у модели во время инференса\n",
        "\n",
        "Задание делится на несколько частей\n",
        "## 1. Реализация (10 Баллов)\n",
        "Напишите реализацию применения SVD к некоторым слоям и некоторым матрицам проекции на выбор. Эта реализация может быть разной, вы можете добавить функцию внутрь класса БЕРТа и внутрь его проекций, которая добавляет им функциональности. Либо это может быть функция которая итерируется по слоям и подменяет их на новый кастомный слой, который поддерживает SVD разложение и оптимальный инференс с его помощью (нужный порядок умножения матриц на активации).\n",
        "Эта реализация должна поддерживать возможность выбирать какие слои сжимать и какие матрицы в них\n",
        "\n",
        "## 2. Чувствительность (6 Баллов)\n",
        "Примените SVD к некоторым слоям и некоторым матрицам. Обратите внимание, что они по разному будут чувствительны к разложению (вспомните график из статьи LASER с лекции). Протестируйте какие слои и какие матрицы более или менее чувствительны к разложению. Сделайте выводы что лучше сжимать, а что лучше не трогать и насколько сильно можно сжимать.\n",
        "\n",
        "## 3. Дообучение (7 Баллов)\n",
        "Выберите несколько лучших постановок из прошлого пункта, эти постановки должны отличаться по количеству параметров и степени сжатия. Зафиксировав эти слои и эти матрицы, далее дообучите их на тренировочной выборке SST2. Рекомендуется пользоваться приведенными выше функциями, они позволят вам сосредоточиться на разложении, а не на рутинных функциях обучения.\n",
        "\n",
        "По итогу предлагается исследовать как просело или выросло качество по сравнению с изначальным, а также как оно зависит от степени сжатия и количества параметров.\n",
        "\n",
        "## 4. Выводы (2 Балла)\n",
        "Сделайте выводы на основе полученных в этом задании результатов, попробуйте понять почему так получилось. Будет интересно и здорово, если Ваши выводы совпадут с какой-нибудь статьей и вы сошлетесь на нее.\n",
        "\n"
      ],
      "metadata": {
        "id": "ANpp8NIswI8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets accelerate tensorly"
      ],
      "metadata": {
        "id": "r7ToXaNzHm-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from packaging import version\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers import AutoConfig, BertForSequenceClassification, BertTokenizer,Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from datasets import load_dataset, load_metric\n",
        "\n"
      ],
      "metadata": {
        "id": "LBH4C-FTHHdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Первым делом обозначим активацию, которая будет у нас почти везде. В случае BERT это у нас GELU, в описании к функции небольшая справка и ссылка на статью"
      ],
      "metadata": {
        "id": "40Tc8pz_Zwng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELUActivation(nn.Module):\n",
        "    \"\"\"\n",
        "    Original Implementation of the GELU activation function in Google BERT repo when initially created. For\n",
        "    information: OpenAI GPT's GELU is slightly different (and gives slightly different results): 0.5 * x * (1 +\n",
        "    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) This is now written in C in nn.functional\n",
        "    Also see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gelu_python: bool = False):\n",
        "        super().__init__()\n",
        "        if use_gelu_python:\n",
        "            self.act = self._gelu_python\n",
        "        else:\n",
        "            self.act = nn.functional.gelu\n",
        "\n",
        "    def _gelu_python(self, input: Tensor) -> Tensor:\n",
        "        return input * 0.5 * (1.0 + torch.erf(input / math.sqrt(2.0)))\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return self.act(input)"
      ],
      "metadata": {
        "id": "p4Y8A5gLKlKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следом нам предстоит регулярно делать эмбеддинг к нашей модели. У Берта 3 эмбеддинга\n",
        "1. для токена. Он нам нужен чтобы правильно кодировать токены\n",
        "2. для позиции. Он нам нужен чтобы кодировать позицию. Позиционные токены иногда делают синусом и косинусом, но часто эффективнее просто их учить.\n",
        "3. для типа токена. Такой вид токена был нужен в оригинале для задачи NSP, нам он не будет нужен\n",
        "\n"
      ],
      "metadata": {
        "id": "eKbIYu_AZ8-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\n",
        "            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        past_key_values_length: int = 0,\n",
        "    ) -> torch.Tensor:\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "8TqsYpkwHNO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следом нам следует сделать механизм внимания. У нас будет несколько голов, проекция для ключей, запросов и значений в виде линейных слоев. Далее потребуется еще одна выходная проекция, она будет в отдельном классе с нормализацией и дропаутом.\n",
        "\n",
        "В реализации может встречаться условие на то является ли текущий слой частью декдора. В нашем случае не является, нам потребуется только энкодер, эти части можно игнорировать."
      ],
      "metadata": {
        "id": "xx4U_qkXbavp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config, position_embedding_type=None):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = position_embedding_type or getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\"\n",
        "        )\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        use_cache = past_key_value is not None\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "wpH-neu0HTie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config, position_embedding_type=None):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(\n",
        "            config, position_embedding_type=position_embedding_type\n",
        "        )\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "CUbO6_AeI_uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ну и конечно нам нужен Feed Forward слой, для него у нас будет проекция в более высокую размерность а потом обратно в размерность нашей модели"
      ],
      "metadata": {
        "id": "b9r7JJr3d50S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = GELUActivation()\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "zGaHslBBKRpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все вышеперечисленные слои мы собираем в один слой берта. Еще раз отметим, что есть часть имплементации под бертовый слой на декодер, не обращайте на него внимания, он нам не потребуется"
      ],
      "metadata": {
        "id": "twGHkIeXtFKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = BertAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            if not self.is_decoder:\n",
        "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
        "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n",
        "                    \" by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        layer_output = self.feed_forward_chunk(attention_output)\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n"
      ],
      "metadata": {
        "id": "0Q5-0DBwMaxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее мы делаем полный энкодер, который будет составлен из нескольких слоев берта. И также пуллер, сверху на 0 токене будет голова в виде линейного слоя."
      ],
      "metadata": {
        "id": "4tA1Ay-ftmsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        output_hidden_states: Optional[bool] = False,\n",
        "        return_dict: Optional[bool] = True,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "            )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "        )\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n"
      ],
      "metadata": {
        "id": "whtXRLqYNP6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ну и наконец полная модель, она соберет вместе эмбеддинги, энкодер и пуллер."
      ],
      "metadata": {
        "id": "oZeDBBjCuAJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n",
        "    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
        "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
        "\n",
        "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
        "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
        "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    _no_split_modules = [\"BertEmbeddings\"]\n",
        "\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "\n",
        "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.position_embedding_type = config.position_embedding_type\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
        "\n",
        "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n",
        "\n",
        "\n",
        "        encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = [None] * self.config.num_hidden_layers #self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "\n",
        "    def get_extended_attention_mask(\n",
        "        self, attention_mask: Tensor, input_shape: Tuple[int], device: torch.device = None, dtype: torch.float = None\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
        "\n",
        "        Arguments:\n",
        "            attention_mask (`torch.Tensor`):\n",
        "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
        "            input_shape (`Tuple[int]`):\n",
        "                The shape of the input to the model.\n",
        "\n",
        "        Returns:\n",
        "            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n",
        "        \"\"\"\n",
        "        if dtype is None:\n",
        "            dtype = torch.float\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        if attention_mask.dim() == 3:\n",
        "            extended_attention_mask = attention_mask[:, None, :, :]\n",
        "        elif attention_mask.dim() == 2:\n",
        "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
        "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
        "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "            extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n",
        "            )\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and the dtype's smallest value for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n",
        "        return extended_attention_mask\n"
      ],
      "metadata": {
        "id": "uj4xOfCzPBEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подобно реализации из HF у нас будет класс именно под классификацию, который использует основную модель и применяет к ней линейный слой. Также на форварде есть возможность сразу посчитать лосс если предоставить лейблы в качестве аргумента."
      ],
      "metadata": {
        "id": "hTFwk4QkVfVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "       # self.post_init()\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        output = (logits,) + outputs[2:]\n",
        "        return ((loss,) + output) if loss is not None else output"
      ],
      "metadata": {
        "id": "3dFWjTdVO3td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы возьмем модель уже обученную под SST2, маленького размера (чтобы было приятнее вычислять). И подгрузим веса в нашу реализацию. Веса доступны по ссылке\n",
        "https://drive.google.com/file/d/1N4RGGFYmw7hVy6TR8SOyu82QmLbB20qy/view?usp=sharing\n",
        "\n",
        "Нам здесь потребовалась собственная реализация для того, чтобы мы могли ее модифицировать далее"
      ],
      "metadata": {
        "id": "lwkuYvvFuj3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained('VityaVitalich/bert-tiny-sst2')\n",
        "\n",
        "bert = BertForSequenceClassification(config=config)\n",
        "\n",
        "state_dict = torch.load('bert_tiny_sst.pt')\n",
        "bert.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "SIhAf5FsHvIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже пример кода, который подгружает токенайзер, мы будем пользоваться стандартным и изменять его не будем.\n",
        "\n",
        "Далее мы создаем трейн и тест сет, которые пойдут в HF Trainer на этапе обучения и тестирования"
      ],
      "metadata": {
        "id": "EL5YcjY8u3q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('VityaVitalich/bert-tiny-sst2')\n",
        "\n",
        "def create_data(tokenizer):\n",
        "  train_set = load_dataset('sst2', split='train').remove_columns(['idx'])\n",
        "  val_set = load_dataset('sst2', split='validation').remove_columns(['idx'])\n",
        "\n",
        "  def tokenize_func(examples):\n",
        "      return tokenizer(examples[\"sentence\"], max_length=256, padding='longest', truncation=True)\n",
        "\n",
        "  encoded_dataset_train = train_set.map(tokenize_func, batched=True)\n",
        "  encoded_dataset_test = val_set.map(tokenize_func, batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer)\n",
        "  return encoded_dataset_train, encoded_dataset_test, data_collator\n",
        "\n",
        "encoded_dataset_train, encoded_dataset_test, data_collator = create_data(tokenizer)\n"
      ],
      "metadata": {
        "id": "2Mn5JxzTGvaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А также подгрузим метрику из библиотеки datasets. Это обычный Accuracy, мы могли бы его и сами сделать по итогу, но зачем изобретать велосипед. Такая функция передается в HF Trainer и вызывается каждый раз, когда мы хотим посчитать метрику"
      ],
      "metadata": {
        "id": "ZkKrUBf4vFUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric('accuracy')\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n"
      ],
      "metadata": {
        "id": "V9qTyeNlKEta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объявим аргументы для тренировки и сам Trainer. Обращаю внимание, что аргументов у него очень очень много, их можно и нужно менять, чтобы добиться более хорошего качества.\n",
        "\n",
        "Рекомендую обратить внимание на некоторые вещи в первую очередь\n",
        "1. Learning rate\n",
        "2. Batch size\n",
        "3. Точность вычислений\n",
        "4. Scheduler warmup steps\n",
        "5. Weight Decay\n",
        "6. Train epochs"
      ],
      "metadata": {
        "id": "tKFbwLwkvXKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    load_best_model_at_end=True,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.1,\n",
        "    fp16=False,\n",
        "    fp16_full_eval=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    seed=42,\n",
        "    save_strategy = \"epoch\",\n",
        "    save_total_limit=5,\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bert,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset_train,\n",
        "    eval_dataset=encoded_dataset_test,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "GlGYIbQqH3jC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже примеры обучения и тестирования нашей модели, которыми удобно пользоваться, чтобы не задумываться о рутинных функциях в обучении"
      ],
      "metadata": {
        "id": "d_WQFgvtv5uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.evaluate(encoded_dataset_test)"
      ],
      "metadata": {
        "id": "d2u2MP7uvzLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже дан шаблон, который можно руководствоваться для замены слоев. Это просто пример, который можно редактировать. Как минимум на данном этапе он не сможет поддерживать разные ранги для разных проекций."
      ],
      "metadata": {
        "id": "1eFZzt_nQBIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_linear_with_svd(model, rank):\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # инициализируйте новый класс с нужными параметрами и замените в исходном берте\n",
        "        else:\n",
        "            replace_linear_with_svd(module, rank)\n",
        "    return model\n",
        "\n",
        "class SVDLinear(nn.Module):\n",
        "    def __init__(self,): # иницилизируйте класс с необходимыми параметрами\n",
        "        super(SVDLinear, self).__init__()\n",
        "\n",
        "        # сделайте сингулярное разложение и запишите матрицы из него в параметры\n",
        "        # не забывайте что мы хотим хранить их эффективно\n",
        "\n",
        "    def forward(self, input):\n",
        "        # напишите форвард который будет эффективно использовать матрицы из разложения"
      ],
      "metadata": {
        "id": "KaGa5E-DQJNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2 (5 Баллов)\n",
        "\n",
        "Данная секция более экспериментальная и направлена на то, чтобы познакомиться с реализацией тензорных разложений.\n",
        "\n",
        "Выберите небольшое подмножество слоев из прошлого задания, которые хорошо сжимаются. Реализуйте поддержку тензорных разложений вместо сингулярного (скорее всего придется сделать решейп и разложение). Посмотрите как изменилось качество, какой выигрыш у SVD по количеству параметров, а также насколько степень разложения влияет на качество в данном случае.\n",
        "\n",
        "На выбор вам предлагается использовать либо Tucker Decomposition либо Tensor Train.\n",
        "\n",
        "На семинаре мы не успели посмотреть на тензорные разложения, внизу пример кода, который демонстрирует как ими можно пользоваться."
      ],
      "metadata": {
        "id": "m9XtcYjezMLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorly as tl\n",
        "from tensorly.decomposition import tucker, tensor_train"
      ],
      "metadata": {
        "id": "mADHM5UzXMFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tl.set_backend('pytorch')\n",
        "\n",
        "# создаем случайный тензор\n",
        "tensor = torch.randn(3, 4, 5)\n",
        "\n",
        "# Применяем Tucker с рангами 2,2,2 (это ранги Core Tensor)\n",
        "core, factors = tucker(tensor, rank=[2, 2, 2])\n"
      ],
      "metadata": {
        "id": "KSAE6lreXV9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Видим что размер Core Tensor такая как мы хотели, а остальные факторы будут растягивать его до изначальной размерности\n",
        "core.size(), [factor.size() for factor in factors]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsXWQGpdXZfO",
        "outputId": "2c40f1c6-aaac-4500-81ec-2d03886e3884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 2, 2]),\n",
              " [torch.Size([3, 2]), torch.Size([4, 2]), torch.Size([5, 2])])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проводим тензорное разложение с рангом 2\n",
        "factors = tensor_train(tensor, rank=2).factors"
      ],
      "metadata": {
        "id": "-Sld9siYXiSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# мы получили 3 тензора, у которых ранг 2, а другие размерности должны соединяться между собой при перемножении.\n",
        "# в случае тензорного разложения, мы можем решейпать в более высокие размерности, допустим тензор размерности 5, тогда каждый фактор разложения будет меньше рангом\n",
        "print(\"Factor tensors:\")\n",
        "for factor in factors:\n",
        "    print(factor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Itcxqlh2X_G1",
        "outputId": "791c3d46-8882-4fdc-d0a1-3c8c4ac0d36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Factor tensors:\n",
            "torch.Size([1, 3, 2])\n",
            "torch.Size([2, 4, 2])\n",
            "torch.Size([2, 5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Бонус (5 баллов)\n",
        "\n",
        "Попробуйте дообучить тензорные разложения, посмотрите как зависит качество от параметров, насколько это выгоднее SVD с точки зрения параметров и полученного качества"
      ],
      "metadata": {
        "id": "vL--rqPs28_9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVkzJuz93I9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}