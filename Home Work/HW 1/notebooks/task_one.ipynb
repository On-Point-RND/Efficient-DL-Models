{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1da2SfM2_O9"
   },
   "source": [
    "# Домашняя работа часть 1  (20 баллов): анализ падения точности LLM после квантизации и спарсификации\n",
    "\n",
    "\n",
    "В этом задании вам предстоит проанализировать популярные методы компресии больших языковых моделей, среди которых QUIK, Wanda и SparseGPT, комбинация SparseGPT и GPTQ\n",
    "\n",
    "В качестве языковой модели будет использована модель opt-350m.\n",
    "\n",
    "Результат задания: Построить 8 графиков (по одному для каждого из 4 методов компресии и двух тестовых датасетов) зависимости точности модели от уровня сжатия в 0, 2 и 4 раза.\n",
    "\n",
    "В виде решения нужно будет сдать ноутбук с построенными графиками и выводами, в частности ответить на вопрос какой метод компрессии выбрать для сохранения наилучшего качества.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AV3IRNDy28hR"
   },
   "source": [
    "## Подготовка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Okd5YfuDPhXh",
    "outputId": "660e2c55-88d7-4a19-9e72-d6d9bdb78cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.29.1-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m570.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: xxhash, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, accelerate\n",
      "Successfully installed accelerate-0.29.1 datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzJEFVTS07og",
    "outputId": "bbcad3aa-3939-4965-8bcf-e465179c2c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lm-eval\n",
      "  Downloading lm_eval-0.4.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.29.1)\n",
      "Collecting evaluate (from lm-eval)\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (2.18.0)\n",
      "Collecting jsonlines (from lm-eval)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm-eval) (2.9.0)\n",
      "Collecting peft>=0.2.0 (from lm-eval)\n",
      "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pybind11>=2.6.2 (from lm-eval)\n",
      "  Downloading pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytablewriter (from lm-eval)\n",
      "  Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge-score>=0.0.4 (from lm-eval)\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting sacrebleu>=1.5.0 (from lm-eval)\n",
      "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (1.2.2)\n",
      "Collecting sqlitedict (from lm-eval)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (2.2.1+cu121)\n",
      "Collecting tqdm-multiprocess (from lm-eval)\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval) (4.38.2)\n",
      "Collecting zstandard (from lm-eval)\n",
      "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from lm-eval) (0.3.8)\n",
      "Collecting word2number (from lm-eval)\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from lm-eval) (10.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm-eval) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm-eval) (24.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm-eval) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm-eval) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm-eval) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm-eval) (0.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (0.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm-eval) (3.9.3)\n",
      "Collecting responses<0.19 (from evaluate->lm-eval)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval) (1.16.0)\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm-eval)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval) (0.9.0)\n",
      "Collecting colorama (from sacrebleu>=1.5.0->lm-eval)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval) (4.9.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm-eval) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm-eval) (12.4.127)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm-eval) (0.15.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm-eval) (23.2.0)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval) (67.7.2)\n",
      "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm-eval)\n",
      "  Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval)\n",
      "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval)\n",
      "  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval)\n",
      "  Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval)\n",
      "  Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
      "Collecting typepy[datetime]<2,>=1.3.2 (from pytablewriter->lm-eval)\n",
      "  Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval) (4.0.3)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval) (5.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->lm-eval) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->lm-eval) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->lm-eval) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->lm-eval) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval) (2023.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm-eval) (2.1.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm-eval) (8.1.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->lm-eval) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->lm-eval) (1.3.0)\n",
      "Building wheels for collected packages: rouge-score, sqlitedict, word2number\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=1da5e2acda41652f8e2919ecf6fbaca4cc2e7cbb991d2926bbed4d69b0497257\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=5892270068b5d47b44119c63fa692507098312a1a472d06ca4338794ac10f267\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=f2ab00c02b24cd73ac22c285a00fe18190513c11649c0ab5187f1d7fa7d44eff\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
      "Successfully built rouge-score sqlitedict word2number\n",
      "Installing collected packages: word2number, sqlitedict, zstandard, tcolorpy, pybind11, portalocker, pathvalidate, mbstrdecoder, jsonlines, colorama, typepy, tqdm-multiprocess, sacrebleu, rouge-score, responses, DataProperty, tabledata, peft, evaluate, pytablewriter, lm-eval\n",
      "Successfully installed DataProperty-1.0.1 colorama-0.4.6 evaluate-0.4.1 jsonlines-4.0.0 lm-eval-0.4.2 mbstrdecoder-1.1.3 pathvalidate-3.2.0 peft-0.10.0 portalocker-2.8.2 pybind11-2.12.0 pytablewriter-1.2.0 responses-0.18.0 rouge-score-0.1.2 sacrebleu-2.4.1 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.4 tqdm-multiprocess-0.0.11 typepy-1.3.2 word2number-1.1 zstandard-0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lm-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir spars_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./spars_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git init\n",
    "!git remote add -f origin https://github.com/On-Point-RND/Efficient-DL-models-Seminars.git\n",
    "!git config core.sparseCheckout true\n",
    "!echo \"Home Work/HW 1\" >> .git/info/sparse-checkout\n",
    "!git pull origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vzTkTQvR3QTn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "SAVING_DIR = '/content/'\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = SAVING_DIR + \"hf_cache/\"\n",
    "os.environ[\"HF_HOME\"] = SAVING_DIR + \"hf_cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3izCW0_CjeV"
   },
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6QqJ0l8Cm9l"
   },
   "source": [
    "https://huggingface.co/facebook/opt-350m <br>\n",
    "\n",
    "OPT: Open Pre-trained Transformer Language Models <br>\n",
    "Семейство моделей с архитектурой GPT-трансформера, выпущенные в начале 2022 года. Обучение моделей выполнялось на корпусах, состоящих преимущественно из текстов на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "966rnVeL51bR",
    "outputId": "01fd856d-39d9-44fc-f0b4-19d35f69d2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'opt-350m'...\n",
      "remote: Enumerating objects: 115, done.\u001b[K\n",
      "remote: Total 115 (delta 0), reused 0 (delta 0), pack-reused 115\u001b[K\n",
      "Receiving objects: 100% (115/115), 558.77 KiB | 4.62 MiB/s, done.\n",
      "Resolving deltas: 100% (59/59), done.\n",
      "Filtering content: 100% (3/3), 1.85 GiB | 60.67 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/facebook/opt-350m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3dL56NJH6cy"
   },
   "source": [
    "## lm-evaluation-harness <br>\n",
    "\n",
    "https://github.com/EleutherAI/lm-evaluation-harness/ <br>\n",
    "\n",
    "harness - единый фреймворк для тестирования эффективности языковых модели на большом количестве различных заданий. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq7H9U8lTE2G"
   },
   "source": [
    "https://huggingface.co/datasets/winogrande <br>\n",
    "\n",
    "WinoGrande - коллекция из 44 тыс. вопросов, вдохновленная Winograd Schema Challenge (Левеск, Дэвис и Моргенштерн, 2011). Каждый вопрос формулируется в виде тестового задания с двумя ответами. Цель выбрать правильный ответ, что требует рассуждений на основе здравого смысла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3uDU9hZTOJd"
   },
   "source": [
    "<img src=\"./images/winogrande.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgaASPLmTygf"
   },
   "source": [
    "https://huggingface.co/datasets/google/boolq\n",
    "\n",
    "BoolQ - набор из 15942 заданий с ответами \"да\"/\"нет\" на вопросы, которые формулируются на основе отрывка текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYH5OTlHTZdN"
   },
   "source": [
    "<img  src=\"./images/boolq.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItamzBY8HVf3",
    "outputId": "d49f4485-2ef0-4245-d925-8a8cffdf37dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-08 14:29:54.142580: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 14:29:54.142636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 14:29:54.144629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-08 14:29:55.705657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-08:14:29:59,006 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-04-08:14:30:04,614 INFO     [__main__.py:335] Selected Tasks: ['boolq', 'winogrande']\n",
      "2024-04-08:14:30:04,614 INFO     [__main__.py:336] Loading selected tasks...\n",
      "2024-04-08:14:30:04,616 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-04-08:14:30:04,675 INFO     [huggingface.py:162] Using device 'cuda'\n",
      "2024-04-08:14:30:06,063 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-04-08:14:30:06,063 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-04-08:14:30:20,146 WARNING  [evaluator.py:222] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-04-08:14:30:20,146 WARNING  [evaluator.py:222] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-04-08:14:30:20,148 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 84507.72it/s]\n",
      "2024-04-08:14:30:20,210 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100% 3270/3270 [00:01<00:00, 1803.46it/s]\n",
      "2024-04-08:14:30:22,144 INFO     [evaluator.py:362] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 9074/9074 [00:41<00:00, 216.54it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/content/opt-350m), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: 4\n",
      "|  Tasks   |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
      "|----------|------:|------|-----:|------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc   |0.5217|±  |0.0140|\n",
      "|boolq     |      2|none  |     0|acc   |0.5761|±  |0.0086|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args \"pretrained=/content/opt-350m\" \\\n",
    "    --tasks winogrande,boolq \\\n",
    "    --batch_size 4 \\\n",
    "    --num_fewshot 0 \\\n",
    "    --device cuda \\ \n",
    "    --trust_remote_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w07cmytIFESp"
   },
   "source": [
    "## Метод квантизации QUIK <br>\n",
    "\n",
    "https://github.com/IST-DASLab/QUIK/tree/master <br>\n",
    "https://arxiv.org/pdf/2310.09259.pdf <br>\n",
    "\n",
    "<img src=\"./images/quik.png\">\n",
    "\n",
    "Метод GPTQ выполняет послойную квантизацию модели с компенсацией ошибки квантизации путем обновления еще не квантизованных весов. <br>\n",
    "Для каждого линейного слоя матрица весов $\\mathbf{W}$ квантизуется последовательно по столбцам, при этом еще не квантизованные веса обновляются, чтобы компенсировать возникшую ошибку:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\mathbf{W}_{update} = \\mathbf{W} + \\delta \\mathbf{W} = \\mathbf{W} + \\dfrac{(\\tilde{\\mathbf{W}}_{m} - \\mathbf{W}_{m})}{[\\mathbf{H}^{-1}]_{mm}} \\mathbf{H}^{-1}_{:, m}, \\quad \\mathbf{H} = \\mathbf{X}\\mathbf{X}^{T}\n",
    "\\end{equation}$\n",
    "\n",
    "где $m$ - индекс столбца, который квантизуется, $\\mathbf{X}$ - матрица активаций, $\\tilde{\\mathbf{W}}$ - деквантизованные веса. <br>\n",
    "Для того чтобы ускорить процесс квантизации и не обновлять всю матрицу $\\mathbf{W}$ после квантизации каждого столбца, они объединяются в блоки (стандартный размер 128). При таком подходе, неквантизованные веса обновляются только в рамках одного блока.\n",
    "\n",
    "Метод QUIK - улучшение версия метода GPTQ, в котором в матрицах весов $W$ каждого слоя модели выделяются столбцы outliers. Данные столбцы не квантизуются, что дает возможность существенно повысить качество модели\n",
    "без значительного увеличения в ее размере. В отличии от LLM.int8() столбцы outliers определяются заранее, исходя из максимального значения активаций в каждом слое, расчитанных на калибровочном датасете. Благодаря этому метод QUIK имеет более высокую скорость инференса по сравнению с LLM.int8().\n",
    "\n",
    "\n",
    "Выбор чувствительных к квантизации столбцов в QUIK выбирается, исходя значения активаций $\\mathbf{X}$, посчитанных на калибровочном датасете:\n",
    "\n",
    "$\n",
    "\\mathbf{i} =  \\underset{\\mathbf{i}}{argmax}\\mathbf{X}, \\\\ $\n",
    "где $\\mathbf{i}$ - 128 индексов столбцов, которые не квантизуются и остаются в fp16. \n",
    "\n",
    "Более подробно про методы квантизации GPTQ и LLM.int8() будет рассказано в лекции про оптимизацию LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EYHZpNkJkZF"
   },
   "source": [
    "<em> Описание параметров: </em>\n",
    "\n",
    "`model` - путь к директории, где хранится модель. <br>\n",
    "`path_to_act_scales` - путь к файлу с максимальными значений активаций для каждого слоя. <br>\n",
    "Данный файл может быть получен с помощью `generate_act_scales.py` из репозитория `https://github.com/mit-han-lab/smoothquant/tree/main`. <br>\n",
    "`path_to_save_quant_model` - путь к директории, где будет сохранена модель после квантизации. Модель сохранятся с деквантизованными fp весами. <br>\n",
    "`fp_features` - количество столбцов outliers в каждом слое, которые не будут квантизованы. <br>\n",
    "`a_bits` - уровень битности [4, 8, 16], в котором будут вычислены активации для каждого слоя. Если уровень битности меньше 16, то активации квантизуются. Но активации, соответствующие столбцам outliers, остаются всегда в fp16/bf16. <br>\n",
    "`w_bits` - уровень квантизации [4, 8, 16] весов модели. сли уровень битности 16, то веса не квантизуются. <br>\n",
    "`w_clip` - индикатор, указывающий, что коэффициент $\\alpha$, который используется для масштабирования весов модели $\\dfrac{\\mathrm{abs}(\\mathbf{W})}{\\alpha}$ при квантизации, будет найден путем подбора, исходя из минимизации ошибки квантизации $ \\lVert \\mathbf{W} - \\tilde{\\mathbf{W}} \\rVert_{2}$, где $\\tilde{\\mathbf{W}}$ - деквантизованные веса. Поскольку в этом случае значение $\\alpha$ может быть меньше, чем максимальное значение весов $\\mathbf{W}$, то веса модели после квантизации $(2^{\\mathrm{w\\_bits} - 1} - 1)\\dfrac{\\mathbf{W}}{\\alpha}$ ограничиваются до промежутка $[-2^{\\mathrm{w\\_bits} - 1},\\, 2^{\\mathrm{w\\_bits} - 1} - 1]$. <br>\n",
    "`dataset` - калиброчовный датасет, который используется для вычисления матрицы Гессе $\\mathbf{H}$ для каждого слоя модели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTJKiwp0557K",
    "outputId": "f5d071f2-61b2-4aec-b4e7-a2e3af622544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Namespace(model='/content/opt-350m', path_to_save_quant_model='/content/weight/opt_350_w4_a16', path_to_act_scales='/content/basic_sparse_quant/quik/experiments/act_scales/opt_350m.pt', dataset='wikitext2', seed=0, nsamples=128, percdamp=0.01, fp_features=128, a_bits=16, w_bits=4, w_clip=True, w_asym=False, sparsity=0, prunen=0, prunem=0, wandb=False, wandb_name='anonymous', int8_2_4=False, smoothquant=False, synthetic_data=False, sparseGPT=False)\n",
      "Loading /content/opt-350m Model...\n",
      "Loaded act_scales from:  /content/basic_sparse_quant/quik/experiments/act_scales/opt_350m.pt\n",
      "Downloading readme: 100% 10.5k/10.5k [00:00<00:00, 29.0MB/s]\n",
      "Downloading data: 100% 733k/733k [00:00<00:00, 3.62MB/s]\n",
      "Downloading data: 100% 6.36M/6.36M [00:00<00:00, 54.1MB/s]\n",
      "Downloading data: 100% 657k/657k [00:00<00:00, 9.93MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 113547.59 examples/s]\n",
      "Generating train split: 100% 36718/36718 [00:00<00:00, 732715.09 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 613068.85 examples/s]\n",
      "Starting ...\n",
      "Ready.\n",
      "\n",
      "Layer: 0: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 1: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 2: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 3: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 4: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 5: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 6: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 7: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 8: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 9: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 10: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 11: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 12: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 13: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 14: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 15: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 16: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 17: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 18: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 19: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 20: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 21: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 22: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 \n",
      "Layer: 23: self_attn.k_proj  self_attn.v_proj  self_attn.q_proj  self_attn.out_proj  fc1  fc2 wikitext2\n",
      "Layers: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\n",
      "WIKITEXT2 PPL: 22.465\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python /content/spars_quant/Home\\ Work/HW\\ 1/quik/experiments/fake_quant/opt.py \\\n",
    "    --model /content/opt-350m \\\n",
    "    --path_to_act_scales /content/spars_quant/Home\\ Work/HW\\ 1/quik/experiments/act_scales/opt_350m.pt \\\n",
    "    --path_to_save_quant_model /content/weight/opt_350_w4_a16 \\\n",
    "    --fp_features 128 \\\n",
    "    --a_bits 16 \\\n",
    "    --w_bits 4 \\\n",
    "    --w_clip \\\n",
    "    --dataset wikitext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mbx8j0B552r",
    "outputId": "2c4de9f2-df27-4364-e006-73b5d303378e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-08 13:42:48.634355: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 13:42:48.634412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 13:42:48.743202: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-08 13:42:50.942531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 22.9MB/s]\n",
      "2024-04-08:13:42:55,570 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-04-08:13:43:02,695 INFO     [__main__.py:335] Selected Tasks: ['boolq', 'winogrande']\n",
      "2024-04-08:13:43:02,696 INFO     [__main__.py:336] Loading selected tasks...\n",
      "2024-04-08:13:43:02,700 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-04-08:13:43:02,760 INFO     [huggingface.py:162] Using device 'cuda'\n",
      "2024-04-08:13:43:03,886 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-04-08:13:43:03,886 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Downloading data: 100% 3.85M/3.85M [00:00<00:00, 16.0MB/s]\n",
      "Downloading data: 100% 1.31M/1.31M [00:00<00:00, 19.4MB/s]\n",
      "Downloading data: 100% 1.31M/1.31M [00:00<00:00, 17.7MB/s]\n",
      "Generating train split: 100% 9427/9427 [00:00<00:00, 275225.38 examples/s]\n",
      "Generating validation split: 100% 3270/3270 [00:00<00:00, 296106.87 examples/s]\n",
      "Generating test split: 100% 3245/3245 [00:00<00:00, 303596.09 examples/s]\n",
      "Downloading data: 100% 2.06M/2.06M [00:00<00:00, 11.2MB/s]\n",
      "Downloading data: 100% 118k/118k [00:00<00:00, 1.75MB/s]\n",
      "Downloading data: 100% 85.9k/85.9k [00:00<00:00, 1.38MB/s]\n",
      "Generating train split: 100% 40398/40398 [00:00<00:00, 1056678.04 examples/s]\n",
      "Generating test split: 100% 1767/1767 [00:00<00:00, 642675.61 examples/s]\n",
      "Generating validation split: 100% 1267/1267 [00:00<00:00, 500865.52 examples/s]\n",
      "2024-04-08:13:43:14,289 WARNING  [evaluator.py:222] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-04-08:13:43:14,289 WARNING  [evaluator.py:222] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-04-08:13:43:14,292 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 77665.49it/s]\n",
      "2024-04-08:13:43:14,353 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100% 3270/3270 [00:01<00:00, 1828.43it/s]\n",
      "2024-04-08:13:43:16,265 INFO     [evaluator.py:362] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 9074/9074 [02:42<00:00, 55.73it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/content/weight/opt_350_w4_a16), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: 4\n",
      "|  Tasks   |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
      "|----------|------:|------|-----:|------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc   |0.5328|±  |0.0140|\n",
      "|boolq     |      2|none  |     0|acc   |0.5651|±  |0.0087|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args \"pretrained=/content/weight/opt_350_w4_a16\" \\\n",
    "    --tasks winogrande,boolq \\\n",
    "    --batch_size 4 \\\n",
    "    --num_fewshot 0 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_i-RnVaLaO6R"
   },
   "source": [
    "## Метод спарсификации SparseGPT\n",
    "\n",
    "https://github.com/IST-DASLab/sparsegpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcIdikzNcT7-"
   },
   "source": [
    "SparseGPT - метод спарсификации LLM, основанные на процедуре Optimal Brain Surgeon (OBS). Оценка значимости каждого веса $w_{m}$ в матрице весов $\\mathbf{W}$ выполняется, исходя из его влияния на функцию Лагранжа:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\dfrac{1}{2} \\dfrac{w_{m}^{2}}{[\\mathbf{H}^{-1}]_{mm}}.\n",
    "\\end{equation}\n",
    "\n",
    "Согласно теории OBS функция Лагранжа $L$ связана с ошибкой $E$, вызванной занулением веса $w_{m}$:\n",
    "\n",
    "$\\begin{equation}\n",
    "E = \\lVert \\mathbf{W}\\mathbf{X} - \\tilde{\\mathbf{W}}\\mathbf{X} \\rVert_{2}^{2},\n",
    "\\end{equation}$\n",
    "где $\\tilde{\\mathbf{W}}$ - матрица весов после зануления $w_{m}$.\n",
    "\n",
    "После расчета значимости каждого веса в выбранном слое, выполняется последовательное (слево-направо) обнуления малозначимых весов $w_{m}$. По аналогии с GPTQ обнуление весов выполняется по столбцам, которые сгруппированы в блоки.\n",
    "При этом ошибка, вызыванная спарсификацией, компенсируется путем калибровки весов, находящихся в одной строке с зануленными весами $\\mathbf{w}_{m}$:\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathbf{w}_{update} = \\mathbf{w} + \\delta \\mathbf{w} = \\mathbf{w} + \\dfrac{\\mathbf{w}_{m}}{[\\mathbf{H}^{-1}]_{mm}} \\mathbf{H}^{-1}_{:, m}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em> Описание параметров </em>\n",
    "`model` - путь к директории, где хранится модель. <br>\n",
    "`dataset` - калибровочный датасет, который используется для расчета матрицы Гессе $\\mathbf{H}$. <br>\n",
    "`sparsity` - коэффициент спарсификации, в интервале от 0.0 (оригинальная модель) до 1.0.<br>\n",
    "`wbits` - уровень битности для спарсифированной модели [4, 8, 16]. Если уровень битности меньше 16, то\n",
    "вместе со спарсификацией будет выполнена также квантизация модели методом GPTQ. Если уровень спрасификации 0.0, то будет выполнена только квантизация методом GPTQ. <br>\n",
    "`sparsity_type` - тип спарсификации. <br>\n",
    "`save_model` - путь к директории, в которой будет сохранена модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python  /content/spars_quant/Home\\ Work/HW\\ 1/sparsegpt/opt.py \\\n",
    "    --model /content/opt-350m \\\n",
    "    --dataset wikitext2 \\\n",
    "    --sparsity 0.5 \\\n",
    "    --wbits 16 \\\n",
    "    --save /content/weights/opt350m_sparsegpt_50_w16_a16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args \"pretrained=/content/weights/opt350m_sparsegpt_50_w16_a16\" \\\n",
    "    --tasks winogrande,boolq \\\n",
    "    --batch_size 4 \\\n",
    "    --num_fewshot 0 \\\n",
    "    --device cuda \\ \n",
    "    --trust_remote_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример запуска скрипта для квантизации модели методом GPTQ до уровня 4 бит (wbits = 4). <br>\n",
    "Для того чтобы одновременно выполнить квантизацию и спарсификацию, необходимо указать коэффициент \n",
    "спарсификации (напр. sparsity = 50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Starting ...\n",
      "Ready.\n",
      "0 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.63\n",
      "error 44.008323669433594\n",
      "0 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.49\n",
      "error 4.0060224533081055\n",
      "0 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 62.487083435058594\n",
      "0 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 0.01483080256730318\n",
      "0 fc1\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 20920.123046875\n",
      "0 fc2\n",
      "Pruning ...\n",
      "time 1.95\n",
      "error 431.683349609375\n",
      "1 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 6478.5068359375\n",
      "1 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 3204.97998046875\n",
      "1 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.52\n",
      "error 5883.50244140625\n",
      "1 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 137.14459228515625\n",
      "1 fc1\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 26005.505859375\n",
      "1 fc2\n",
      "Pruning ...\n",
      "time 1.86\n",
      "error 465.0400085449219\n",
      "2 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 8643.05859375\n",
      "2 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 4585.8134765625\n",
      "2 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 8700.955078125\n",
      "2 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 137.20111083984375\n",
      "2 fc1\n",
      "Pruning ...\n",
      "time 0.53\n",
      "error 29832.021484375\n",
      "2 fc2\n",
      "Pruning ...\n",
      "time 1.85\n",
      "error 634.217041015625\n",
      "3 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 9821.669921875\n",
      "3 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 5620.56982421875\n",
      "3 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 10236.17578125\n",
      "3 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 111.35760498046875\n",
      "3 fc1\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 32414.12109375\n",
      "3 fc2\n",
      "Pruning ...\n",
      "time 1.91\n",
      "error 777.0628662109375\n",
      "4 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 10524.4765625\n",
      "4 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 5604.033203125\n",
      "4 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.49\n",
      "error 10852.865234375\n",
      "4 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 147.43319702148438\n",
      "4 fc1\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 34189.640625\n",
      "4 fc2\n",
      "Pruning ...\n",
      "time 1.90\n",
      "error 856.2066650390625\n",
      "5 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 8980.736328125\n",
      "5 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 5495.81982421875\n",
      "5 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 9479.9609375\n",
      "5 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 148.20614624023438\n",
      "5 fc1\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 35737.0859375\n",
      "5 fc2\n",
      "Pruning ...\n",
      "time 1.89\n",
      "error 936.3609619140625\n",
      "6 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 11236.03515625\n",
      "6 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 5101.87890625\n",
      "6 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 10515.849609375\n",
      "6 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 272.6003112792969\n",
      "6 fc1\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 38198.31640625\n",
      "6 fc2\n",
      "Pruning ...\n",
      "time 1.87\n",
      "error 1142.35693359375\n",
      "7 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 12707.6357421875\n",
      "7 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 5808.421875\n",
      "7 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 11633.4580078125\n",
      "7 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 415.2646484375\n",
      "7 fc1\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 41878.31640625\n",
      "7 fc2\n",
      "Pruning ...\n",
      "time 1.91\n",
      "error 1491.250244140625\n",
      "8 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 15405.7255859375\n",
      "8 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 6486.78759765625\n",
      "8 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 14121.90625\n",
      "8 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.44\n",
      "error 564.8113403320312\n",
      "8 fc1\n",
      "Pruning ...\n",
      "time 0.44\n",
      "error 43362.83203125\n",
      "8 fc2\n",
      "Pruning ...\n",
      "time 1.78\n",
      "error 2086.154296875\n",
      "9 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.44\n",
      "error 15188.51171875\n",
      "9 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 6822.81787109375\n",
      "9 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 14680.6259765625\n",
      "9 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 649.6368408203125\n",
      "9 fc1\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 43559.359375\n",
      "9 fc2\n",
      "Pruning ...\n",
      "time 1.80\n",
      "error 2092.4794921875\n",
      "10 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 15698.412109375\n",
      "10 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 8128.3291015625\n",
      "10 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.44\n",
      "error 15693.0\n",
      "10 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 809.77099609375\n",
      "10 fc1\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 47650.390625\n",
      "10 fc2\n",
      "Pruning ...\n",
      "time 1.87\n",
      "error 2197.06591796875\n",
      "11 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 15634.861328125\n",
      "11 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.51\n",
      "error 9391.291015625\n",
      "11 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.52\n",
      "error 14994.626953125\n",
      "11 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 617.5698852539062\n",
      "11 fc1\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 48887.28515625\n",
      "11 fc2\n",
      "Pruning ...\n",
      "time 1.83\n",
      "error 2495.153076171875\n",
      "12 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 16025.412109375\n",
      "12 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 11267.3759765625\n",
      "12 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 16433.859375\n",
      "12 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 917.1103515625\n",
      "12 fc1\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 52005.9921875\n",
      "12 fc2\n",
      "Pruning ...\n",
      "time 1.89\n",
      "error 2356.7998046875\n",
      "13 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 16892.927734375\n",
      "13 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 11332.5458984375\n",
      "13 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 16196.20703125\n",
      "13 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 866.5166015625\n",
      "13 fc1\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 52434.0234375\n",
      "13 fc2\n",
      "Pruning ...\n",
      "time 1.90\n",
      "error 2484.696533203125\n",
      "14 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 16843.55859375\n",
      "14 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.53\n",
      "error 12070.7431640625\n",
      "14 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.55\n",
      "error 16970.240234375\n",
      "14 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.49\n",
      "error 488.303466796875\n",
      "14 fc1\n",
      "Pruning ...\n",
      "time 0.52\n",
      "error 52785.2734375\n",
      "14 fc2\n",
      "Pruning ...\n",
      "time 2.09\n",
      "error 2286.0751953125\n",
      "15 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.51\n",
      "error 17029.8125\n",
      "15 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.51\n",
      "error 11490.837890625\n",
      "15 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.49\n",
      "error 17021.904296875\n",
      "15 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 484.1152038574219\n",
      "15 fc1\n",
      "Pruning ...\n",
      "time 0.51\n",
      "error 51903.11328125\n",
      "15 fc2\n",
      "Pruning ...\n",
      "time 2.19\n",
      "error 2261.796142578125\n",
      "16 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 16649.4765625\n",
      "16 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.51\n",
      "error 12383.970703125\n",
      "16 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.51\n",
      "error 17200.37890625\n",
      "16 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 580.069580078125\n",
      "16 fc1\n",
      "Pruning ...\n",
      "time 0.51\n",
      "error 51366.59765625\n",
      "16 fc2\n",
      "Pruning ...\n",
      "time 1.97\n",
      "error 2254.51611328125\n",
      "17 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 16053.28125\n",
      "17 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 11800.5830078125\n",
      "17 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 16860.498046875\n",
      "17 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 493.6664123535156\n",
      "17 fc1\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 49183.65234375\n",
      "17 fc2\n",
      "Pruning ...\n",
      "time 1.88\n",
      "error 2030.3516845703125\n",
      "18 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 15017.984375\n",
      "18 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 11118.255859375\n",
      "18 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 16148.349609375\n",
      "18 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 320.78912353515625\n",
      "18 fc1\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 46328.3984375\n",
      "18 fc2\n",
      "Pruning ...\n",
      "time 1.94\n",
      "error 1858.00048828125\n",
      "19 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 14877.96875\n",
      "19 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 9636.4755859375\n",
      "19 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.49\n",
      "error 16021.8369140625\n",
      "19 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.49\n",
      "error 444.27630615234375\n",
      "19 fc1\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 43410.72265625\n",
      "19 fc2\n",
      "Pruning ...\n",
      "time 1.92\n",
      "error 1781.455322265625\n",
      "20 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.49\n",
      "error 15545.3154296875\n",
      "20 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 8612.6201171875\n",
      "20 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.52\n",
      "error 15884.73046875\n",
      "20 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.55\n",
      "error 617.635009765625\n",
      "20 fc1\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 43516.14453125\n",
      "20 fc2\n",
      "Pruning ...\n",
      "time 1.98\n",
      "error 2133.1064453125\n",
      "21 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 16058.30078125\n",
      "21 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 8601.2041015625\n",
      "21 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 16472.955078125\n",
      "21 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 632.5169677734375\n",
      "21 fc1\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 44507.52734375\n",
      "21 fc2\n",
      "Pruning ...\n",
      "time 1.92\n",
      "error 2890.033935546875\n",
      "22 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.50\n",
      "error 20772.587890625\n",
      "22 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 9455.8134765625\n",
      "22 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 19038.51953125\n",
      "22 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.47\n",
      "error 1205.01416015625\n",
      "22 fc1\n",
      "Pruning ...\n",
      "time 0.48\n",
      "error 42150.828125\n",
      "22 fc2\n",
      "Pruning ...\n",
      "time 1.79\n",
      "error 3175.6630859375\n",
      "23 self_attn.k_proj\n",
      "Pruning ...\n",
      "time 0.45\n",
      "error 17554.923828125\n",
      "23 self_attn.v_proj\n",
      "Pruning ...\n",
      "time 0.43\n",
      "error 12782.1953125\n",
      "23 self_attn.q_proj\n",
      "Pruning ...\n",
      "time 0.43\n",
      "error 16936.80078125\n",
      "23 self_attn.out_proj\n",
      "Pruning ...\n",
      "time 0.44\n",
      "error 1199.06982421875\n",
      "23 fc1\n",
      "Pruning ...\n",
      "time 0.46\n",
      "error 35352.99609375\n",
      "23 fc2\n",
      "Pruning ...\n",
      "time 1.78\n",
      "error 2898.63037109375\n",
      "model.decoder.embed_tokens.weight tensor(5.8277e-07)\n",
      "model.decoder.embed_positions.weight tensor(0.0005)\n",
      "model.decoder.project_out.weight tensor(0.)\n",
      "model.decoder.project_in.weight tensor(0.)\n",
      "model.decoder.layers.0.self_attn.k_proj.weight tensor(0.0127)\n",
      "model.decoder.layers.0.self_attn.k_proj.bias tensor(0.)\n",
      "model.decoder.layers.0.self_attn.v_proj.weight tensor(0.1703)\n",
      "model.decoder.layers.0.self_attn.v_proj.bias tensor(0.)\n",
      "model.decoder.layers.0.self_attn.q_proj.weight tensor(0.0696)\n",
      "model.decoder.layers.0.self_attn.q_proj.bias tensor(0.)\n",
      "model.decoder.layers.0.self_attn.out_proj.weight tensor(0.2514)\n",
      "model.decoder.layers.0.self_attn.out_proj.bias tensor(0.)\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight tensor(0.)\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias tensor(0.)\n",
      "model.decoder.layers.0.fc1.weight tensor(0.1587)\n",
      "model.decoder.layers.0.fc1.bias tensor(0.)\n",
      "model.decoder.layers.0.fc2.weight tensor(0.1761)\n",
      "195.240567445755\n",
      "wikitext2\n",
      "Evaluating ...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Perplexity: 23.961563\n"
     ]
    }
   ],
   "source": [
    "!python  /content/spars_quant/Home\\ Work/HW\\ 1/sparsegpt/opt.py \\\n",
    "    --model /content/opt-350m \\\n",
    "    --dataset wikitext2 \\\n",
    "    --sparsity 0.0 \\\n",
    "    --wbits 4 \\\n",
    "    --save /content/weights/opt350m_gptq_w4_a16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args \"pretrained=/content/weights/opt350m_gptq_w4_a16\" \\\n",
    "    --tasks winogrande,boolq \\\n",
    "    --batch_size 4 \\\n",
    "    --num_fewshot 0 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiucXEveX_8Y"
   },
   "source": [
    "## Метод спарсификации Wanda\n",
    "\n",
    "https://github.com/locuslab/wanda?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCSbCtjVYESS"
   },
   "source": [
    "Wanda - метод неструктурной спарсификации LLM, в котором ранжирование значимости весов $\\mathbf{W}$ каждого слоя модели выполняется посредством метрики:\n",
    "\n",
    "$\\begin{align}\n",
    "score(\\mathbf{W}) = \\lvert \\mathbf{W} \\rvert \\cdot \\lVert \\mathbf{X} \\rVert_{2},\n",
    "\\end{align}$\n",
    "где $\\lVert \\mathbf{X} \\rVert_{2}$ - усредненные по всем строкам посредством $L_{2}$ нормы значения активаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihs45jLtZOlX"
   },
   "source": [
    "<img src=\"./images/wanda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em> Описание параметров </em>\n",
    "`model` - путь к директории, где хранится модель. <br>\n",
    "`prune_method` - метод спарсификации. <br>\n",
    "`sparsity_ratio` - коэффициент спарсификации, в интервале от 0.0 (оригинальная модель) до 1.0. <br>\n",
    "`sparsity_type` - тип спарсификации. <br>\n",
    "`save_model` - путь к директории, в которой будет сохранена модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNw6A9xd55ze",
    "outputId": "747c4080-3264-4025-aef1-ce54e2dbb3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "torch 2.2.1+cu121\n",
      "transformers 4.38.2\n",
      "accelerate 0.29.1\n",
      "# of gpus:  1\n",
      "loading llm model /content/opt-350m\n",
      "use device  cuda:0\n",
      "pruning starts\n",
      "loading calibdation data\n",
      "dataset loading complete\n",
      "pruning layer 0 name self_attn.k_proj\n",
      "pruning layer 0 name self_attn.v_proj\n",
      "pruning layer 0 name self_attn.q_proj\n",
      "pruning layer 0 name self_attn.out_proj\n",
      "pruning layer 0 name fc1\n",
      "pruning layer 0 name fc2\n",
      "pruning layer 1 name self_attn.k_proj\n",
      "pruning layer 1 name self_attn.v_proj\n",
      "pruning layer 1 name self_attn.q_proj\n",
      "pruning layer 1 name self_attn.out_proj\n",
      "pruning layer 1 name fc1\n",
      "pruning layer 1 name fc2\n",
      "pruning layer 2 name self_attn.k_proj\n",
      "pruning layer 2 name self_attn.v_proj\n",
      "pruning layer 2 name self_attn.q_proj\n",
      "pruning layer 2 name self_attn.out_proj\n",
      "pruning layer 2 name fc1\n",
      "pruning layer 2 name fc2\n",
      "pruning layer 3 name self_attn.k_proj\n",
      "pruning layer 3 name self_attn.v_proj\n",
      "pruning layer 3 name self_attn.q_proj\n",
      "pruning layer 3 name self_attn.out_proj\n",
      "pruning layer 3 name fc1\n",
      "pruning layer 3 name fc2\n",
      "pruning layer 4 name self_attn.k_proj\n",
      "pruning layer 4 name self_attn.v_proj\n",
      "pruning layer 4 name self_attn.q_proj\n",
      "pruning layer 4 name self_attn.out_proj\n",
      "pruning layer 4 name fc1\n",
      "pruning layer 4 name fc2\n",
      "pruning layer 5 name self_attn.k_proj\n",
      "pruning layer 5 name self_attn.v_proj\n",
      "pruning layer 5 name self_attn.q_proj\n",
      "pruning layer 5 name self_attn.out_proj\n",
      "pruning layer 5 name fc1\n",
      "pruning layer 5 name fc2\n",
      "pruning layer 6 name self_attn.k_proj\n",
      "pruning layer 6 name self_attn.v_proj\n",
      "pruning layer 6 name self_attn.q_proj\n",
      "pruning layer 6 name self_attn.out_proj\n",
      "pruning layer 6 name fc1\n",
      "pruning layer 6 name fc2\n",
      "pruning layer 7 name self_attn.k_proj\n",
      "pruning layer 7 name self_attn.v_proj\n",
      "pruning layer 7 name self_attn.q_proj\n",
      "pruning layer 7 name self_attn.out_proj\n",
      "pruning layer 7 name fc1\n",
      "pruning layer 7 name fc2\n",
      "pruning layer 8 name self_attn.k_proj\n",
      "pruning layer 8 name self_attn.v_proj\n",
      "pruning layer 8 name self_attn.q_proj\n",
      "pruning layer 8 name self_attn.out_proj\n",
      "pruning layer 8 name fc1\n",
      "pruning layer 8 name fc2\n",
      "pruning layer 9 name self_attn.k_proj\n",
      "pruning layer 9 name self_attn.v_proj\n",
      "pruning layer 9 name self_attn.q_proj\n",
      "pruning layer 9 name self_attn.out_proj\n",
      "pruning layer 9 name fc1\n",
      "pruning layer 9 name fc2\n",
      "pruning layer 10 name self_attn.k_proj\n",
      "pruning layer 10 name self_attn.v_proj\n",
      "pruning layer 10 name self_attn.q_proj\n",
      "pruning layer 10 name self_attn.out_proj\n",
      "pruning layer 10 name fc1\n",
      "pruning layer 10 name fc2\n",
      "pruning layer 11 name self_attn.k_proj\n",
      "pruning layer 11 name self_attn.v_proj\n",
      "pruning layer 11 name self_attn.q_proj\n",
      "pruning layer 11 name self_attn.out_proj\n",
      "pruning layer 11 name fc1\n",
      "pruning layer 11 name fc2\n",
      "pruning layer 12 name self_attn.k_proj\n",
      "pruning layer 12 name self_attn.v_proj\n",
      "pruning layer 12 name self_attn.q_proj\n",
      "pruning layer 12 name self_attn.out_proj\n",
      "pruning layer 12 name fc1\n",
      "pruning layer 12 name fc2\n",
      "pruning layer 13 name self_attn.k_proj\n",
      "pruning layer 13 name self_attn.v_proj\n",
      "pruning layer 13 name self_attn.q_proj\n",
      "pruning layer 13 name self_attn.out_proj\n",
      "pruning layer 13 name fc1\n",
      "pruning layer 13 name fc2\n",
      "pruning layer 14 name self_attn.k_proj\n",
      "pruning layer 14 name self_attn.v_proj\n",
      "pruning layer 14 name self_attn.q_proj\n",
      "pruning layer 14 name self_attn.out_proj\n",
      "pruning layer 14 name fc1\n",
      "pruning layer 14 name fc2\n",
      "pruning layer 15 name self_attn.k_proj\n",
      "pruning layer 15 name self_attn.v_proj\n",
      "pruning layer 15 name self_attn.q_proj\n",
      "pruning layer 15 name self_attn.out_proj\n",
      "pruning layer 15 name fc1\n",
      "pruning layer 15 name fc2\n",
      "pruning layer 16 name self_attn.k_proj\n",
      "pruning layer 16 name self_attn.v_proj\n",
      "pruning layer 16 name self_attn.q_proj\n",
      "pruning layer 16 name self_attn.out_proj\n",
      "pruning layer 16 name fc1\n",
      "pruning layer 16 name fc2\n",
      "pruning layer 17 name self_attn.k_proj\n",
      "pruning layer 17 name self_attn.v_proj\n",
      "pruning layer 17 name self_attn.q_proj\n",
      "pruning layer 17 name self_attn.out_proj\n",
      "pruning layer 17 name fc1\n",
      "pruning layer 17 name fc2\n",
      "pruning layer 18 name self_attn.k_proj\n",
      "pruning layer 18 name self_attn.v_proj\n",
      "pruning layer 18 name self_attn.q_proj\n",
      "pruning layer 18 name self_attn.out_proj\n",
      "pruning layer 18 name fc1\n",
      "pruning layer 18 name fc2\n",
      "pruning layer 19 name self_attn.k_proj\n",
      "pruning layer 19 name self_attn.v_proj\n",
      "pruning layer 19 name self_attn.q_proj\n",
      "pruning layer 19 name self_attn.out_proj\n",
      "pruning layer 19 name fc1\n",
      "pruning layer 19 name fc2\n",
      "pruning layer 20 name self_attn.k_proj\n",
      "pruning layer 20 name self_attn.v_proj\n",
      "pruning layer 20 name self_attn.q_proj\n",
      "pruning layer 20 name self_attn.out_proj\n",
      "pruning layer 20 name fc1\n",
      "pruning layer 20 name fc2\n",
      "pruning layer 21 name self_attn.k_proj\n",
      "pruning layer 21 name self_attn.v_proj\n",
      "pruning layer 21 name self_attn.q_proj\n",
      "pruning layer 21 name self_attn.out_proj\n",
      "pruning layer 21 name fc1\n",
      "pruning layer 21 name fc2\n",
      "pruning layer 22 name self_attn.k_proj\n",
      "pruning layer 22 name self_attn.v_proj\n",
      "pruning layer 22 name self_attn.q_proj\n",
      "pruning layer 22 name self_attn.out_proj\n",
      "pruning layer 22 name fc1\n",
      "pruning layer 22 name fc2\n",
      "pruning layer 23 name self_attn.k_proj\n",
      "pruning layer 23 name self_attn.v_proj\n",
      "pruning layer 23 name self_attn.q_proj\n",
      "pruning layer 23 name self_attn.out_proj\n",
      "pruning layer 23 name fc1\n",
      "pruning layer 23 name fc2\n",
      "******************************\n",
      "layer 0 sparsity 0.500000\n",
      "layer 1 sparsity 0.500000\n",
      "layer 2 sparsity 0.500000\n",
      "layer 3 sparsity 0.500000\n",
      "layer 4 sparsity 0.500000\n",
      "layer 5 sparsity 0.500000\n",
      "layer 6 sparsity 0.500000\n",
      "layer 7 sparsity 0.500000\n",
      "layer 8 sparsity 0.500000\n",
      "layer 9 sparsity 0.500000\n",
      "layer 10 sparsity 0.500000\n",
      "layer 11 sparsity 0.500000\n",
      "layer 12 sparsity 0.500000\n",
      "layer 13 sparsity 0.500000\n",
      "layer 14 sparsity 0.500000\n",
      "layer 15 sparsity 0.500000\n",
      "layer 16 sparsity 0.500000\n",
      "layer 17 sparsity 0.500000\n",
      "layer 18 sparsity 0.500000\n",
      "layer 19 sparsity 0.500000\n",
      "layer 20 sparsity 0.500000\n",
      "layer 21 sparsity 0.500000\n",
      "layer 22 sparsity 0.500000\n",
      "layer 23 sparsity 0.500000\n",
      "sparsity sanity check 0.5000\n",
      "******************************\n",
      "evaluating on wikitext2\n",
      "nsamples 140\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "wikitext perplexity 35.49598693847656\n"
     ]
    }
   ],
   "source": [
    "!python /content/spars_quant/Home\\ Work/HW\\ 1/wanda/main_opt.py \\\n",
    "    --model /content/opt-350m \\\n",
    "    --prune_method wanda \\\n",
    "    --sparsity_ratio 0.5 \\\n",
    "    --sparsity_type unstructured \\\n",
    "    --save_model /content/weight/opt350m_wanda_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aOg0GOv55s7",
    "outputId": "509f9634-df59-4574-c374-bbc54082a3a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-08 13:59:43.446479: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 13:59:43.446526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 13:59:43.447934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-08 13:59:44.645927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-08:13:59:47,889 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-04-08:13:59:54,375 INFO     [__main__.py:335] Selected Tasks: ['boolq', 'winogrande']\n",
      "2024-04-08:13:59:54,375 INFO     [__main__.py:336] Loading selected tasks...\n",
      "2024-04-08:13:59:54,377 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-04-08:13:59:54,436 INFO     [huggingface.py:162] Using device 'cuda'\n",
      "2024-04-08:13:59:55,463 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-04-08:13:59:55,463 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-04-08:14:00:02,319 WARNING  [evaluator.py:222] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-04-08:14:00:02,319 WARNING  [evaluator.py:222] Overwriting default num_fewshot of boolq from None to 0\n",
      "2024-04-08:14:00:02,321 INFO     [task.py:395] Building contexts for winogrande on rank 0...\n",
      "100% 1267/1267 [00:00<00:00, 49623.06it/s]\n",
      "2024-04-08:14:00:02,422 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
      "100% 3270/3270 [00:01<00:00, 1788.22it/s]\n",
      "2024-04-08:14:00:04,391 INFO     [evaluator.py:362] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100% 9074/9074 [02:40<00:00, 56.49it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/content/weight/opt350m_wanda_50), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: 4\n",
      "|  Tasks   |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
      "|----------|------:|------|-----:|------|-----:|---|-----:|\n",
      "|winogrande|      1|none  |     0|acc   |0.5012|±  |0.0141|\n",
      "|boolq     |      2|none  |     0|acc   |0.4737|±  |0.0087|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args \"pretrained=/content/weight/opt350m_wanda_50\" \\\n",
    "    --tasks winogrande,boolq \\\n",
    "    --batch_size 4 \\\n",
    "    --num_fewshot 0 \\\n",
    "    --device cuda \\ \n",
    "    --trust_remote_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uA0R5dm8in9"
   },
   "source": [
    "# Ваши графики и решения тут:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
